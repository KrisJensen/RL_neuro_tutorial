\begin{figure*}[!t]
    \centering
    \vspace*{-0.5em}
    \includegraphics[width=0.85\textwidth]{./figs/TD.pdf}
    \caption[TD]{\label{fig:TD}
        %
        {\bfseries Temporal difference learning.}
        {\bfseries (A)}~Value functions aquired through temporal difference learning (\Cref{eq:TD-learning}) while acting according to either a random (top) or an optimal (bottom) policy.
        Dark blue indicates negative expected reward (-100) and yellow indicates positive expected reward (+50).
        For these simulations, we used a learning rate of $\delta = 0.05$ and no temporal discount factor ($\gamma = 1$).
        Under the random policy, states near the cliff have low value even though they are close to the goal, since the agent will often fall off the cliff from there.
        Under the optimal policy, all states have high expected reward, since the agent will always find the goal.
        States nearer the goal have higher value than those further away, although this is hard to distinguish on this color scale.
        {\bfseries (B)}~Empirical reward as a function of episode number for a TD-learning agent that acts according to \Cref{eq:value_action_selection} while updating its value estimates according to \Cref{eq:TD-learning}.
        For this agent, action selection assumes access to a `one-step' world model in order to evaluate the consequence of each putative action.
        We see that the agent gradually converges to an optimal policy.
        Parameters for the agent are as in (A).
        {\bfseries (C)}~Value function learned by a greedy TD agent as in (B), plotted either early (top) or late (bottom) in training.
        For this analysis, we used a high learning rate of $\delta = 0.5$ to make the early TD updates larger and therefore more visible.
        }
    \vspace*{-1.0em}
\end{figure*}

