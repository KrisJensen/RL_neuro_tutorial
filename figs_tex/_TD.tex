\begin{figure*}[!t]
    \centering
    \vspace*{-0.5em}
    \includegraphics[width=0.95\textwidth]{./figs/TD.pdf}
    \caption[TD]{\label{fig:TD}
        %
        {\bfseries Temporal difference learning.}
        {\bfseries (A)}~Value functions aquired through temporal difference learning (\Cref{eq:TD-learning}) while acting according to either a random (top) or an optimal (bottom) policy.
        These simulations were performed with a random start state in the cliffworld environment to ensure full coverage of the space.
        Dark blue indicates negative expected reward (-100) and yellow indicates positive expected reward (+50).
        For these simulations, we used a learning rate of $\alpha = 0.05$ and no temporal discounting ($\gamma = 1$).
        Under the random policy, states near the cliff have low value even though they are close to the goal, since the agent will often fall off the cliff from there.
        Under the optimal policy, all states have high expected reward, since the agent will always find the goal.
        States nearer the goal have higher value than those further away, although this is hard to distinguish on this color scale.
        {\bfseries (B)}~Empirical reward as a function of episode number for a TD-learning agent that acts according to \Cref{eq:value_action_selection} while updating its value estimates according to \Cref{eq:TD-learning}.
        For this agent, action selection assumes access to a `one-step' world model in order to evaluate the consequence of each putative action.
        We see that the agent gradually converges to an optimal policy.
        Parameters for the agent are as in (A).
        {\bfseries (C)}~Value function learned by a greedy TD agent as in (B), plotted either early (top) or late (bottom) in training.
        Early in training, the agent has learned that the cliff is bad but doesn't know where the goal is or how to get there.
        Late in training, the agent has learned a value function that locally resembles the optimal value function from (A), while it has not learned the value of distant states that are rarely or never visited.
        This is a potential shortcoming of `greedy' agents that can easily converge to a sub-optimal local maximum in more complicated environments.
        For this analysis, we used a high learning rate of $\alpha = 0.5$ to make the early TD updates larger and therefore more visible.
        }
    \vspace*{-1.0em}
\end{figure*}

