\begin{figure*}[!t]
    \centering
    \vspace*{-0.5em}
    \includegraphics[width=0.95\textwidth]{./figs/MB_rev.pdf}
    \caption[MB]{\label{fig:MB}
        %
        {\bfseries Model-based reinforcement learning.}
        {\bfseries (A)}~Learning curves for model-based (MB) and model-free (MF) RL agents.
        The MB agent used a depth-first search to compute an optimal path at each decision point, gradually learning the reward and transition functions from exploring the environment.
        The MF agent was a simple Q-learning agent with $\epsilon = 0$ and learning rate $\alpha = 1$.
        {\bfseries (B)}~Wallclock time needed to run 100 episodes of cliffworld with either the MB or MF agents from (A), as a function of the length of the environment.
        While the MB agent required less experience to learn a good policy, the wallclock time per episode was much larger than for the MF agent.
        This illustrates an important balance between model-based and model-free reinforcement learning, where MF methods usually require more experience but MB methods require more compute at decision time.
        {\bfseries (C)}~Learning curve for an agent using the successor representation (SR) together with learning curves for the model-based agent in (A) and the greedy TD-agent from \Cref{fig:TD}.
        The goal was moved from location (9, 0) to location (0, 4) at episode 40 (vertical black line), and location (9, 0) was instead given a reward of -5.
        The MB and SR agents had their reward functions updated to reflect this change and rapidly adapted their policies, while the TD agent had no such mechanism for robustness to changing reward functions.
        Reward curves were convolved with a Gaussian kernel ($\sigma = 3$ episodes), which is why performance appears to decrease slightly before episode 40 for the TD agent.
        The TD and SR agents were assumed to have access to a 1-step world model at initialization, while the MB agent learned the transition structure from experience.
        This is why the MB agent does not exhibit faster initial learning than the SR and TD agents.
        The MB agent also exhibits a small drop in performance around the change in reward because the agents were greedy and did not all learn the full transition function.
        {\bfseries (D)}~An SR agent cannot always adapt to a new reward function if the newly rewarded states have low probablity under the old policy.
        Value functions are shown for an agent that learned an initial policy in an environment with a small reward in the upper left state and intermediate reward in the upper right state (left column).
        A new large reward was then introduced in either the top left state (top row) or bottom left state (bottom row).
        In the second case, the agent was unable to adapt since the old policy had low probability of reaching the large reward, even after initially going to the left.
        This results in a low expected value for going left from the start state (red circles).
        {\bfseries (E)}~Learning curve for a standard Q-learning agent (blue) or DYNA agents that perform different numbers of Q value updates for each step of physical action (legend).
        The DYNA updates simply used cached experience rather than data from a learned world model.
        DYNA agents are able to make better use of limited experience at the cost of increased compute (proportional to the number of updates).
        }
    \vspace*{-1.0em}
\end{figure*}

