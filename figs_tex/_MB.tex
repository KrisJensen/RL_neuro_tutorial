\begin{figure*}[!t]
    \centering
    \vspace*{-0.5em}
    \includegraphics[width=0.80\textwidth]{./figs/MB.pdf}
    \caption[MB]{\label{fig:MB}
        %
        {\bfseries Model-based reinforcement learning.}
        {\bfseries (A)}~Learning curves for model-based (MB) and model-free (MF) RL agents.
        In this case, the MB agent used a depth-first search to compute an optimal path at each decision point.
        If a shorter path to a given state had already been discovered, the search was not continued.
        The reward and transition functions of the agent were initialized to zero and the empty set respectively, and they were gradually populated as the agent explored the environment across episodes.
        The MF agent was a simple Q-learning agent with $\epsilon = 0$ and $\delta = 1$.
        The MB agent requires substantially less experience than the MF agent for a given level of performance.
        {\bfseries (B)}~Wallclock time needed to run 100 episodes of cliffworld with either the MB or MF agents from (A), as a function of the length of the environment.
        Although (A) shows that the MB agent requires substantially less experience to learn a good policy, the wallclock time per episode was much larger than that required by the MF agent.
        This illustrates an important balance between model-based and model-free reinforcement learning, where MF methods usually require more experience but MB methods require more compute for a given level of experience.
        In domains such as robotics, where collecting data is very time consuming, MB methods can still be a more efficient way to reach a high level of performance.
        {\bfseries (C)}~Learning curve for an agent using the successor representation (SR) together with learning curves for the model-based agent in (A) and the greedy TD-agent from \Cref{fig:TD}.
        At episode 40 (vertical black line), the goal was moved from location (9, 0) to location (0, 4), and location (9, 0) was instead given a reward of -5.
        The MB and SR agents had their reward functions updated to reflect this change and were immediable able to adapt their policies, while the TD agent had no such mechanism for robustness to changing reward functions.
        It is worth noting that an SR agent cannot always adapt to a new reward function if the newly rewarded states have low probablity under the old policy, since the successor matrix itself is a function of the policy.
        Reward curves were convolved with a Gaussian kernel ($\sigma = 3$ episodes), which is why performance appears to decrease slightly before episode 40 for the TD agent.
        In this simulation, the TD and SR agents were assumed to have access to a 1-step world model at initialization, while the MB agent had to learn the transition structure from experience.
        This is why the MB agent does not exhibit faster initial learning than the SR and TD agents.
        {\bfseries (D)}~Learning curve for a standard Q-learning agent (blue) or DYNA agents that perform different numbers of Q value updates for each step of physical action (legend).
        In this case, the DYNA updates simply used cached experience rather than data from a learned world model.
        We see that DYNA agents are able to make better use of limited experience, although this comes at the cost of increased compute (proportional to the number of updates).
        }
    \vspace*{-1.0em}
\end{figure*}

