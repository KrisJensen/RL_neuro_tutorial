
\subsubsection*{Policy gradient methods}

A conceptually simpler approach for deep reinforcement learning uses policy gradient methods \citep{sutton2018reinforcement}, where a neural network with parameters $\theta$ takes as input the (observable) state of the environment and directly outputs a policy $\pi_\theta$.
This has also found more support and use in the neuroscience literature than deep Q-learning, where policy gradient methods have recently attracted significant interest as models of learning and neural dynamics in the biological brain \citep{wang2018prefrontal, jensen2023recurrent, merel2019deep, song2017reward}.

The objective in a policy gradient network is to find the setting of $\theta$ that maximizes expected reward.
A naive way to achieve this would be to define $R_\tau := \sum_{t=0}^T \gamma^t r_t$ and compute gradients given by
\begin{align}
    \nabla_\theta J(\theta) &= \nabla_\theta \mathbb{E}_{\tau \sim p_{\pi_\theta}(\tau)} \left [ R_\tau \right ]\\
    &= \sum_{\tau} R_\tau \nabla_\theta p_{\pi_\theta}(\tau).
    \label{eq:simple_deriv}
\end{align}
Here, $\tau \sim p_{\pi_\theta}(\tau)$ indicates trajectories sampled from the distribution induced by the policy $\pi_\theta$, and $J(\theta)$ indicates the expectation of $R_\tau$ under $p_{\pi_\theta}(\tau)$ (c.f. \Cref{eq:RL_objective}).
However, evaluating \Cref{eq:simple_deriv} requires us to know how the environment will respond to our actions, which in general may not be the case.
Instead, we use the `log-derivative trick', which takes advantage of the linearity of the expectation and the identity $\nabla_\theta \log f(\theta) = f(\theta)^{-1} \nabla_\theta f(\theta)$ to write
\begin{align}
    \label{eq:deriv_J}
    \nabla_\theta J(\theta) & = \sum_\tau R_\tau \nabla_\theta p_{\pi_\theta}(\tau) \\
                            & = \sum_\tau R_\tau p_{\pi_\theta}(\tau) \nabla_\theta \log p_{\pi_\theta}(\tau) \\
                            & = \mathbb{E}_{\tau \sim p_{\pi_\theta}(\tau)} \left [ R_\tau \nabla_\theta \log p_{\pi_\theta}(\tau) \right ],
\end{align}
Since the environment does not depend on $\theta$, we can simplify the calculation of $\nabla_\theta \log p_{\pi_\theta} (\tau)$:
\begin{align}
    \label{eq:deriv_log_ptau}
    \nabla_\theta \log p_{\pi_\theta}(\tau) & = \nabla_\theta \left [ \log p(s_0) + \sum_{t=0}^T \log p(s_{t+1} | s_t, a_t) + \log \pi_\theta (a_t|s_t) \right ] \\
                                      & = \sum_{t=0}^T \nabla_\theta \log \pi_\theta (a_t|s_t).
\end{align}
Inserting \Cref{eq:deriv_log_ptau} in \Cref{eq:deriv_J} and taking a Monte Carlo estimate of the expectation, we arrive at the REINFORCE algorithm \citep{williams1992simple}:
\begin{align}
    \nabla_\theta J(\theta) & = \mathbb{E}_{\tau \sim p_{\pi_\theta}(\tau)} \left [ R_\tau \sum_{t=0}^T \nabla_\theta \log \pi_\theta (a_t|s_t) \right ]                                         \\
                            & \approx \frac{1}{N} \sum_{\tau \sim p_{\pi_\theta}(\tau)} \left ( \sum_{t=0}^T \gamma^t r_t \right ) \left ( \sum_{t=0}^T \nabla_\theta \log \pi_\theta (a_t|s_t) \right ),
    \label{eq:orig_reinforce}
\end{align}
where the second line approximates the expectation with $N$ empirical rollouts of the policy in the environment.
Intuitively, \Cref{eq:orig_reinforce} says that we should preferentially upregulate the probability of trajectories with high reward.
Importantly, \Cref{eq:orig_reinforce} no longer requires differentiation through the environment -- only the policy.

While the REINFORCE algorithm is unbiased, it also has high variance, which can make learning slow and unstable.
It is therefore common to introduce modifications, which can help reduce the variance.
The first of these comes from noting that an action taken at time $t$ cannot affect the reward received at times $t'<t$.
This allows us to define $R_t := \sum_{t'=t}^T \gamma^{t'-t} r_{t'}$ and write a new update rule as
\begin{equation}
    \label{eq:reinforce}
    \hat{\nabla}_\theta J(\theta) \approx \frac{1}{N} \sum_{\tau \sim p_{\pi_\theta}(\tau)}  \sum_{t=0}^T R_t \nabla_\theta \log \pi_\theta (a_t|s_t).
\end{equation}
This is the formulation most commonly used in the literature, but it is not actually the same as \Cref{eq:orig_reinforce}, which would require us to use $R_t = \sum_{t'=t}^T \gamma^{t'-0} r_{t'}$.
As briefly discussed in \Cref{sec:problem_setting}, this is because the discount factor $\gamma$ is generally used as a variance reduction method rather than because we intrinsically care less about rewards later in the task.
In fact, \Cref{eq:reinforce} is not strictly speaking a gradient \citep{nota2019policy}, which is why denote it $\hat{\nabla}$.

It can also be shown that subtracting an action-independent baseline from $R_t$ does not change the expectation in \Cref{eq:reinforce}, while potentially reducing its variance.
A common choice here is the expected future reward $V(s_t)$, which gives rise to the so-called `actor-critic' algorithm
\begin{equation}
    \label{eq:AC}
    \hat{\nabla}_\theta J(\theta) \approx \frac{1}{N} \sum_{\tau \sim p_{\pi_\theta}(\tau)}  \sum_{t=0}^T (R_t - V(s_t)) \nabla_\theta \log \pi_\theta (a_t|s_t).
\end{equation}
Intuitively, \Cref{eq:AC} upregulates the probability of actions that lead to higher-than-expected reward and downregulates the probability of actions that lead to lower-than-expected reward.
Interestingly, a rich neuroscience literature suggests a biological implementation of an actor-critic-like algorithm, with dorsal striatum implementing the `actor' and ventral striatum the `critic' \citep{takahashi2008silencing,sutton2018reinforcement}.

Finally, it is common to reduce the variance of our gradient estimator through an approach known as `bootstrapping', which uses the approximation $R_t \approx r_t + \gamma V(s_{t+1})$.
This is useful because $r_t + \gamma V(s_{t+1})$ has lower variance than $R_t$, which can reduce the noise in our gradient estimate when using finite sample sizes.
We therefore replace $(R_t - V(s_t))$ in \Cref{eq:AC} with the `advantage function' $A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ to implement the `advantage actor-critic' (A2C) algorithm.
While we have considered two extreme cases of a full Monte Carlo estimate of $R_t$ and a `one-step' bootstrap, the sum in $R_t$ can be truncated to any order with $R_{t'}$ replaced by $V(s_{t'})$ \citep{sutton2018reinforcement}.
In theory, this estimator remains unbiased if our value function is correct.
In practice the estimate of $V(s_{t'})$ learned by the critic will be inexact, which introduces a bias to our parameter updates.
Bootstrapping therefore leads to a tradeoff between the bias and variance of parameter updates.

For these actor-critic algorithms, it is common to parameterize both the policy $\pi_\theta(a|s)$ and value function $V_\theta(s)$ with neural networks.
To optimize these parameters using out-of-the-box automatic differentiation, we need to write down an `objective function' with the correct gradients -- but we saw in \Cref{eq:simple_deriv} that this cannot simply be the expected reward.
Instead, we define an auxiliary utility (i.e. negative loss)
\begin{equation}
    \label{eq:Jtilde}
    \tilde{J}(\theta) = \frac{1}{N} \sum_{\tau \sim p_{\pi_\theta}(\tau)}  \sum_{t=0}^T (R_t - V(s_t)) \log \pi_\theta (a_t|s_t),
\end{equation}
where $R_t$ can optionally be aproximated by $r_t + \gamma V(s_{t+1})$.
While $\tilde{J}(\theta)$ has no intrinsic interpretation, it is chosen such that $\nabla_\theta \tilde{J}(\theta) = \hat{\nabla}_\theta J(\theta)$ when treating $\delta_t := (R_t - V(s_t))$ as constant w.r.t $\theta$, and the gradients can be computed using standard automatic differentiation.
The gradient of the value function loss is then given by $\nabla_\theta \mathcal{L}_V = \nabla_\theta \sum_t \frac12  (R_t - V_\theta(s_t))^2 = \sum_t \left [ - \delta_t \nabla_\theta V_\theta(s_t) \right ]$.

\input{./figs_tex/_metaRL.tex}

While these policy gradient methods may seem far removed from neuroscience, they have had several applications in the field in recent years \citep{wang2018prefrontal, jensen2023recurrent, merel2019deep,li2022integrating,song2017reward}.
Of particular interest, \citet{wang2018prefrontal} suggested that frontal cortex can be well described as a \emph{recurrent} deep RL agent, where the RNN parameters are configured by learning from rewards over long periods of time from many tasks that have a shared underlying structure.
Importantly, this `slow' model-free learning process gives rise to an agent that can rapidly learn from experience with \emph{fixed parameters} when exposed to a new task from the same distribution.
This is achieved by the agent learning to effectively implement a fast RL-like algorithm in the \emph{dynamics} of the network (\Cref{fig:meta}).
This process, whereby an agent trained slowly on a large distribution of tasks can rapidly adapt to a new task, is known as `meta-reinforcement learning' \citep{finn2017model, ritter2018been, duan2016rl, wang2016learning}.
\citet{wang2018prefrontal} showed that such as meta-RL model can explain a range of neuroscientific findings, including
\begin{itemize}
    \item Dynamic adaptation of the effective learning rate of an agent to the volatility of the environment \citep{behrens2007learning}.
    \item The emergence of `model-based' behaviour in the `two-step' task commonly used to distinguish between model-free and model-based RL \citep{miller2017dorsal,daw2011model}.
    \item The ability of animals to get progressively faster at learning when exposed to multiple tasks with a consistent abstract task structure \citep{harlow1949formation}.
\end{itemize}
Further evidence for this meta-RL framework comes from \citet{hattori2023meta}, who showed that across-session learning in a reversal learning task relied on synaptic plasticity in orbitofrontal cortex (OFC; a subregion of PFC), while within-session learning relied on recurrent dynamics in OFC.
Recently, \citet{jensen2023recurrent} also extended the work of \citet{wang2018prefrontal} to allow the meta-RL network dynamics to also update the policy from imagined experience using a learned model of the environment -- reminiscent of DYNA, but now implemented in RNN dynamics instead of parameter updates.

%While the policy gradient methods discussed above are useful, they also have limitations.
%One is that most policy gradient methods are fundamentally `on-policy', which means that the data used for learning must be sampled from the agent itself.
%This is necessary for the Monte Carlo estimate to be unbiased -- although `off-policy' policy gradient methods also exist, which de-bias the gradients e.g. through the use of importance sampling \citep{espeholt2018impala,jie2010connection,peshkin2002learning,haarnoja2018soft}.
%Another is that the policy gradients can have fairly high variance despite all of our variance reduction efforts.
