\section{The successor representation}
\label{sec:SR}

As we saw in the previous section, an important distinction can be made between model-free reinforcement learning methods, which cache stimulus-response mappings based on prior experience, and model-based reinforcement learning methods, which compute a policy by simulating possible futures using a world model at decision-time.
However, we have also noted how animals both need the flexibility of model-based methods as well as the rapid decision making afforded by model-free methods.
It has therefore been suggested in a range of works that animals use either both or an intermediate method that combines some model-free and some model-based features.
A particularly prevalent theory has been that of the `successor representation' (SR), which has been proposed to explain both human behaviour \citep{momennejad2017successor} and features of neural activity \citep{stachenfeld2017hippocampus}.
In particular, the SR allows for flexible adaptation to changing reward functions without having to perform expensive simulations at decision time.

\citet{dayan1993improving} introduced the theory underlying the SR, which rewrites the expected reward starting from state $s$ as
\begin{align}
    V_\pi(s) &= \mathbb{E}_\pi \left [ \sum_{t = 0} \gamma^t r_t | s_0 = s\right ] \\
    &= \sum_{t = 0} \gamma^t \sum_{s'} p_\pi(s_t = s' | s_0 = s) r(s')\\
    &= \sum_{s'} r(s') \sum_t \gamma^t p_\pi(s_t = s' | s_0 = s)\\
    &= \b{r}^T \b{m}^\pi_s.
\end{align}
Here, $\b{r}$ is a vector of the reward associated with each state, and $\b{m}^\pi_s$ is a vector of the expected discounted future occupancy of state $s'$ given that the agent starts in state $s$ and follows the policy $\pi$:
\begin{equation}
    M^\pi_{s s'} = \sum_{t = 0} \gamma^t p_\pi(s_t = s' | s_0 = s).
\end{equation}
The full matrix $\b{M}^\pi$, constructed from stacking the $\b{m}^\pi_s$ corresponding to all states $s$, is denoted the `successor matrix', and constructing it allows us to write down a vector of expected rewards from any state $s$ as
\begin{equation}
    \b{v}_\pi = \b{M}^\pi \b{r}.
\end{equation}
Here, we have retained the superscript $\pi$ to indicate that the successor matrix depends on the policy of the agent, which affects the expected occupancy of different states.
Since this allows us to compute the value of each state, we can perform action selection using \Cref{eq:value_action_selection}.

The flexibility of the successor representation arises when the reward structure of the environment changes, $\b{r} \rightarrow \b{r}'$.
We can now compute the expected reward associated with each state under the new reward function and old policy,
\begin{equation}
    \b{v}_\pi' = \b{M}^\pi \b{r}'.
\end{equation}
Of course this new value function will lead to us eventually changing our policy and having to update $\b{M}$, but it provides a better starting point than using the wrong policy \emph{and} value function as in standard temporal-difference learning (\Cref{fig:MB}C).
The successor matrix can be learned by simple temporal-difference learning when transitioning from $s_t$, analogous to \Cref{eq:TD-learning}:
\begin{equation}
    \Delta M^\pi_{s_t s'} \propto -M^\pi_{s_t s'} + I_{s_t = s'} + \gamma M^\pi_{s_{t+1} s'}.
\end{equation}
Here, ${s'}$ is the set of all other states, $s_{t+1}$ is the next state actually observed, and the learning rule says that transitioning from $s_t$ to $s_{t+1}$ means that (i) we have just been in state $s_t$, and (ii) we should increase the expected occupancy of all states commonly reached from $s_{t+1}$ (including $s_{t+1}$ itself).
Alternatively, if the policy-dependent transition matrix $\b{T}^\pi$ is known, where $T^\pi_{s s'} = p_{\pi}(s_{t+1} = s' | s_t = s)$, the successor matrix can be computed as the geometric series $\b{M}^\pi = \b{I} + \gamma \b{T}^\pi + \gamma^2 (\b{T}^\pi)^2 + \ldots = (\b{I} - \gamma \b{T}^\pi)^{-1}$.

The SR has also been proposed as a model of how humans and other animals learn and generalize \citep{momennejad2017successor, stachenfeld2017hippocampus, geerts2020general,gershman2018successor}.
For example, humans have been shown to adapt more readily to changes in reward functions than changes in transition functions in a simple sequential binary decision-making task \citep{momennejad2017successor}, consistent with the SR facilitating rapid adaptation to changes in $\b{r}$ but only slow learning of $\b{M}$.
Additionally, hippocampal `place cells' have been proposed to encode a predictive map, with each cell corresponding to a column of $\b{M}$ \citep{stachenfeld2017hippocampus}.
In this model, the firing of a place cell in a given location $s$ reflects the expected future occupancy of its `preferred' location $s'$ conditioned on currently being at $s$.
\citet{stachenfeld2017hippocampus} showed that the SR model explains a range of findings in the hippocampal literature.
For example, this model explains the asymmetry of place fields on a one-dimensional track \citep{mehta2000experience}, where $s'$ will is more likely to be reached from other states $s$ that \emph{precede} $s'$ than equidistant states that \emph{follow} $s'$.
The SR also explains why place fields change near a newly inserted barrier, since two states on either side of the barrier can no longer be visited in quick succession \citep{alvernhe2011local}.

While the SR is perhaps the most prominent model in systems neuroscience that combines features of model-free and model-based RL, it is not the only one.
Another interesting approach that has found parallels in the neuroscience literature is the `DYNA' architecture of \citet{sutton1991dyna}.
In this framework, a model of the world is learned from experience and used to train a model-free policy offline by bootstrapping imagined experience sampled from the model.
This allows for more data-efficient learning of model-free policies at the cost of additional compute during `rest' but without necessarily needing more compute at decision time, in contrast to the model-based approaches discussed previously (\Cref{fig:MB}D).
The model used to simulate data for offline training can either be an explicit learned world model, or it can simply be a memory buffer of past experiences in the form of $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$ tuples.
Such experience replay has proven crucial to the success of modern deep reinforcement learning agents by allowing for higher data efficiency and reducing the instability arising from the autocorrelated nature of online experience \citep{mnih2013playing,schaul2015prioritized}.
A prominent theory in neuroscience suggests that hippocampal replays could be implementing such a DYNA-like algorithm by generating imagined experience used to train the model-free RL systems of the brain \citep{mattar2018prioritized}.
This theory is supported by the finding that the patterns of replay exhibited by rodents in various navigation tasks are consistent with the optimal replays of a Q-learning agent with DYNA, and it has recently been extended to explain not just the content of replay but also their timing \citep{agrawal2022temporal}.
