\section{Additional topics of interest}
\label{sec:additional}

While we have tried to provide a fairly comprehensive overview of topics in reinforcement learning of interest to neuroscience, there are naturally many interesting areas that we have had to omit.
Here we provide a brief description of some of these together with pointers to relevant literature for those who are interested in exploring them further.

\subsection{Auxiliary losses}
\label{sec:auxiliary}
A key challenge in deep reinforcement learning is that of `representation learning' \citep{botvinick2020deep}.
In particular, because rewards are generally sparse, it can be difficult to learn the full processing pipeline from raw inputs to a latent representation conducive to action selection on the basis of rewards alone.
It is therefore common in the machine learning literature to introduce additional `auxiliary costs' to the RL utility function, which use a richer data source to drive learning in the network.
A common example is to include predictive losses that require the agent to predict the next observation from the current state and action \citep{jaderberg2016reinforcement, zintgraf2019varibad}.
Such predictive losses have close parallels in neuroscience, where it has been suggested that predictive learning could result in many of the representations observed in biological circuits \citep{rao1999predictive, stachenfeld2017hippocampus, whittington2020tolman, blanco2021dopamine} and serve as a foundation for model-based planning \citep{jensen2023recurrent}.
This suggests a potentially important interaction between self-supervised representation learning and reward-driven reinforcement learning in biological circuits.

\subsection{Hierarchical reinforcement learning}
\label{sec:HRL}
So far, we have considered a simple environment consisting of states and actions, and all planning and decision making has taken place in the space of action primitives.
However, when planning over longer horizons, it can be necessary to break down the overall policy into a series of sub-goals, sub-policies, or `skills' \citep{sutton1999between, pateria2021hierarchical}.
This is the topic of hierarhical reinforcement learning (HRL) and `options', where an agent learns a high-level policy over policies that can themselves be specified in terms of primitive actions or even lower-level policies.
Such HRL has been found to explain features of human behaviour \citep{eckstein2020computational,botvinick2008hierarchical,botvinick2009hierarchically} and remains an area of substantial interest in the neuroscience literature.

\subsection{Off-policy \& offline reinforcement learning}
\label{sec:off-policy}
In most of the work considered in this paper, the experience used to train the RL agents have been sampled from the policy of the agent itself.
Indeed this is required for the gradients to be unbiased in the policy-gradient setting.
However, an area of substantial interest is that of offline reinforcement learning, where the agent is trained from scratch on the basis of pre-collected experience \citep{levine2020offline}.
This is particularly important in cases where online data collection is expensive or too risky but large-scale datasets exist, such as in many healthcare settings.
Off-policy reinforcement learning is the related problem of learning from a combination of online data and pre-generated data, possibly from a `stale' version of the current agent.
The off-policy setting is especially relevant to biology, where data collection is expensive and we therefore wish to make maximum use of existing data.
This can e.g. be achieved through experience replay, which can be prioritized (instead of sampled on-policy) to maximize future reward and minimize temporal opportunity costs \citep{mattar2018prioritized, agrawal2022temporal,schaul2015prioritized}.

\subsection{Imitation learning}
\label{sec:imitation}
Related to the problem of offline reinforcement learning is that of \emph{imitation learning}, where we also learn from pre-collected data.
However, in contrast to offline RL where we make no assumption about the quality of the policy used to collect the data, imitation learning assumes that the data has been collected by an `expert' we wish to imitate \citep{levine2020offline}.
This is useful in cases where a large amount of expert data is available, such as the case of autonomous driving \citep{pan2017agile}.
Imitation learning is clearly important during early development in biological organisms, where we learn from observing the individuals around us.
Indeed, such imitation learning is a hallmark not just of humans but has also been demonstrated in organisms as `simple' as the bumblebee \citep{loukola2017bumblebees}.

\subsection{Linear reinforcement learning}
\label{sec:linear_RL}
As we have seen in most of this tutorial, reinforcement learning is generally difficult and require iterative algorithms that often scale poorly with the problem size.
However, there are settings where we can simplify the problem to the point where it becomes analytically tractable in an approach known as `linear reinforcement learning' \citep{todorov2006linearly, todorov2009efficient}.
This is similar to the SR approach, where we saw that the value function reduces to a linear function of the reward-per-state.
Similar to how the SR matrix can be seen as describing the dynamics of some `base policy', we also define a base policy in linear RL and compute a `control cost' as the KL divergence between transition dynamics with and without our controller:
\begin{equation}
    \mathcal{L}_{ctrl}(s) = KL \left [ u(s' | s) || p(s'|s) \right ],
\end{equation}
where $p(s' | s)$ are the prior transition dynamics and $u(s' | s)$ are the controlled transition dynamics marginalized over the policy.
For $\mathcal{L}_{ctrl}(s)$ to be well-defined, we require $u(s'|s) = 0$ whenever $p(s'|s) = 0$, which prevents impossible transitions even under our flexible controller.
When subtracting this loss from the RL objective, the resulting utility turns out to be convex in $u$ and can therefore be solved analytically for the controller, which implicitly specifies the policy.
This approach has recently been used as an explicit model of biological decision making \citep{piray2021linear}.
It also has close parallels to learning and planning as inference \citep{levine2018reinforcement, solway2012goal,botvinick2012planning} and to RL with information bottlenecks \citep{lai2021policy}.
Both of these families of approaches also boil down to reinforcement learning with a KL-regularized reward function, and they have also been used as models of biological decision making.

\subsection{Successor features}
\label{sec:SFs}
In \Cref{sec:SR}, we saw that the successor representation can be used as the basis of decision making that flexibly adapts to environments with changing reward structures.
However, we developed this framework only in the tabular setting despite extending TD-learning and Q-learning to the `deep RL' setting with function approximation.
This leaves open the question of whether a similar generalization of the SR exists.
This turns out to be the case and is known as `successor features' (SF; \citealp{barreto2017successor}), where the expected future observation of a given feature of the environment is used in place of the expected future state occupancy.
Successor features have also been shown to have a biologically plausible implementation that facilitates learning and generalization in noisy and partially observable environments \citep{vertes2019neurally}.

%\subsection{Reinforcement learning with human feedback}

%\subsection{Active exploration}

\subsection{Multi-agent reinforcement learning}
\label{sec:multi-agent}
We have only considered the case of single agents interacting with a black-box environment.
However, in many cases, multiple agents are simultaneously interacting with each other and the environment around them \citep{gronauer2022multi}.
This means that, from the point of view of a single agent, the other agents are part of its environment.
In such settings, there are interesting learning dynamics beyond the scope of the present tutorial, but which are covered in detail by e.g. \citet{gronauer2022multi}, and which are also of substantial interest in game theory \citep{nowe2012game}.
In some cases, a whole group of agents may be working together to maximize a single joint reward function -- as is the case for members of a single sports team.
Interestingly, the learning of many individual neurons in the brain from a single common reinforcing signal (such as dopamine) can be modelled as such a multi-agent reinforcement learning problem \citep{sutton2018reinforcement}.
If the `agents' (or neurons) are assumed to have Bernoulli-logistic outputs, \citet{williams1992simple} shows that the independent learning of individual agents from the global reward signal leads to the implementation of a policy gradient algorithm at the population level \citep{sutton2018reinforcement}.
