
\subsubsection*{Value-based methods}

While the policy gradient methods discussed above are useful, they also have limitations.
One is that most policy gradient methods are fundamentally `on-policy', which means that the data used for learning must be sampled from the agent itself.
This is necessary for the Monte Carlo estimate to be unbiased -- although `off-policy' policy gradient methods also exist, which de-bias the gradients e.g. through the use of importance sampling \citep{espeholt2018impala,jie2010connection,peshkin2002learning}.
Another is that the policy gradients can have fairly high variance despite all of our variance reduction efforts.
We can begin to tackle both of these shortcomings using a method known as `deep Q-learning'.
To do so, we first note that our actor-critic algorithm above required us to estimate the value function $V_\phi(s_t)$.
However, as we saw in \Cref{eq:value_action_selection}, such value functions can directly be used for action selection without having to fit a separate policy network.
We also noted previously that this requires simulating the results of the actions, which can be inconvenient.
As in the tabular setting, we can circumvent this problem by directly fitting state-action values (`Q-values'), now using function approximation.
This gives rise to the family of `deep Q-learning' methods, which closely mirror the tabular Q-learning considered previously, but now with function approximation.

The general recipe involves defining a state action value function $Q_\theta(s, a)$, where the parameters $\theta$ of the deep network defining our agent are learned as follows:
\begin{itemize}
    \item Collect experience $(s_t, a_t, r_t, s_{t+1})$.
    \item Define a loss $\mathcal{L} = 0.5 [ Q_\theta(s_t, a_t) - (r_t + \gamma \text{max}_a Q_\theta(s_{t+1}, a)) ]^2 $.
    \item Update the network parameters $\Delta \theta \propto - \frac{\partial \mathcal{L}}{\partial \theta}$.
\end{itemize}
The above steps can either be run `online' using the experience of the agent as it is being generated, or it could occur `in batch', whereby a collection of experiences is first generated, followed by optimization of the Q network.
The `max' in the target value ensures that we continuously improve our policy.
When acting according to our policy, we simply pick the action predicted to have the highest value, usually using some variant of $\epsilon$-greedy or softmax to introduce stochasticity and variability into our trajectories.

On the surface, the above looks like a straightforward generalization of tabular Q-learning, and it may seem surprising that deep Q-learning did not see significant use or success until the foundational work of \citet{mnih2013playing}.
However, a major difficulty arises from the autocorrelation of the states observed by the agent, which leads to the network `chasing' a target function $(r_t + \gamma \text{max}_a Q_\theta(s_{t+1}, a))$ that changes with the environment.
A critical advance that overcame the resulting instability was the use of an experience replay buffer, whereby the experience generated by the agent is added to a global replay buffer $\mathcal{B}$.
One or more experiences are then sampled randomly from the buffer and used to update the network parameters -- reminiscent of the `DYNA' architecture described previously.
An additional algorithmic instability arises from the fact that $\mathcal{L}$ includes the term $\text{max}_a Q_\theta(s_{t+1}, a)$, which we cannot take gradients through despite its dependence on $\theta$.
This means that we are trying to optimize our parameters for a continuously moving target, which destabilitizes the optimization process.
To combat this, it is common to use a `target network' $Q_{\theta'}(s_{t+1}, a)$ that is fixed for a period of time, usually after being set to a copy of our `student network'.
Together, these two approaches give rise to the `deep Q network' (DQN) developed by \citet{mnih2013playing}, which is trained as follows:
\begin{itemize}
    \item Collect experience $(s_t, a_t, r_t, s_{t+1})$ and add to $\mathcal{B}$ [optionally many iterations].
    \item Sample a \emph{new} experience $(s'_t, a'_t, r'_t, s'_{t+1}) \sim \mathcal{B}$ [optionally a full batch].
    \item Define a loss $\mathcal{L} = 0.5 [ Q_\theta(s'_t, a'_t) - (r'_t + \gamma \text{max}_a Q_{\theta'}(s'_{t+1}, a)) ]^2 $ [optionally averaged over the full batch]. Note that we are training the student network with parameters $\theta$ while using a target network with parameters $\theta'$ inside the max.
    \item Update the network parameters $\Delta \theta \propto - \frac{\partial \mathcal{L}}{\partial \theta}$.
    \item Once we have repeated the above a sufficient number of times, set our target network to the student network, $\theta' \leftarrow \theta$.
\end{itemize}
As indicated above, this procedure usually involves averaging over a full batch of experience when computing gradients.
Experience can also be collected in batch, and usually `stale' experiences are periodically removed from $\mathcal{B}$.
It is worth noting that this algorithm is effectively off-policy, since most experience in $\mathcal{B}$ was collected by a policy defined by an old set of parameters $\theta$ -- and the data in $\mathcal{B}$ can in fact be generated completely independently of the agent we are training.
Finally, even though the above is more stable than naive deep Q-learning, an additional instability arises from the fact that $Q_{max}(s'_{t+1}) = \text{max}_a Q_{\theta'}(s'_{t+1}, a)$ uses the same Q values both to estimate which action is best and what the value of that action is, which leads to a positively biased estimate.
This can be mitigated by using \emph{separate} Q networks to select the best action and evaluate its value, $Q_{max}(s'_{t+1}) \leftarrow Q_{\theta'}(s'_{t+1}, \text{argmax}_a(Q_{\theta}(s'_{t+1}, a)))$, in an approach known as `double Q-learning' \citep{van2016deep}.
Commonly, this is done by simply using the student network for action selection and the target network to evaluate its expected value.

While DQNs have shown impressive performance across a range of machine learning settings \citep{mnih2013playing, lillicrap2015continuous, schaul2015prioritized, kalashnikov2018qt}, they have seen less interest from the neuroscience community.
This is perhaps paradoxical given the prevalence of tabular Q-learning in the theoretical neuroscience literature.
An interesting exception to this is recent work by \citet{makino2023arithmetic}, which shows parallels between the values learned by a DQN and neural representations in mammalian cortex during a compositional behavioural task.
Additionally, the importance of experience replay in DQNs \citep{mnih2013playing, schaul2015prioritized} has close parallels to the proposal that hippocampal replay constitutes a form of experience replay \citep{mattar2018prioritized} -- although the latter was only analyzed in a tabular setting.

