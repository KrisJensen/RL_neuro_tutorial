
\subsubsection*{Value-based methods}

The deep RL approaches most similar to the tabular methods considered in \Cref{sec:temporal_difference} and \Cref{sec:q_learning} use neural networks to compute state-action values, which can be used for action selection as we saw in \Cref{eq:value_action_selection}.
However, by using function approximation instead of the tabular values considered previously, these networks can generalize to unseen states in large state spaces.
This gives rise to the family of `deep Q-learning' methods, which closely mirror the tabular Q-learning considered previously, but now with function approximation.

The general recipe involves defining a state action value function $Q_\theta(s, a)$, where the parameters $\theta$ of the deep network defining our agent are learned as follows:
\begin{itemize}
    \item Collect experience $(s_t, a_t, r_t, s_{t+1})$.
    \item Define a loss $\mathcal{L} = 0.5 [ Q_\theta(s_t, a_t) - (r_t + \gamma \text{max}_a Q_\theta(s_{t+1}, a)) ]^2 $.
    \item Update the network parameters $\Delta \theta \propto - \frac{\partial \mathcal{L}}{\partial \theta}$.
\end{itemize}
%The above steps can either be run `online' using the experience of the agent as it is being generated, or it could occur `in batch', whereby a collection of experiences is first generated, followed by optimization of the Q network.
%The `max' in the target value ensures that we continuously improve our policy.
When acting according to our policy, we simply pick the action predicted to have the highest value, usually using some variant of $\epsilon$-greedy or softmax to introduce stochasticity and variability into the training data.

On the surface, the above looks like a straightforward generalization of tabular Q-learning, and it may seem surprising that deep Q-learning did not see significant use or success until the foundational work of \citet{mnih2013playing}.
However, a major difficulty arises from the autocorrelation of the states observed by the agent, which destabilizes training.
This can be mitigated by the use of `experience replay', where the experience generated by the agent is added to a global replay buffer $\mathcal{B}$.
One or more experiences are then sampled randomly from the buffer at each iteartion and used to update the network parameters -- reminiscent of the `DYNA' architecture described previously.
An additional algorithmic instability arises from the term $\text{max}_a Q_\theta(s_{t+1}, a)$, which we cannot differentiate despite its dependence on $\theta$.
%This means that we are trying to optimize our parameters for a continuously moving target, which destabilitizes the optimization process.
It is therefore common to use a `target network' $Q_{\theta'}(s_{t+1}, a)$ that is fixed for a period of time, usually after being set to a copy of our `student network'.
Together, these two approaches give rise to the `deep Q network' (DQN) developed by \citet{mnih2013playing}, which is trained as follows:
\begin{itemize}
    \item Collect experience $(s_t, a_t, r_t, s_{t+1})$ and add to $\mathcal{B}$ [optionally many iterations and optionally remove stale experiences].
    \item Randomly sample an experience $(s'_t, a'_t, r'_t, s'_{t+1}) \sim \mathcal{B}$ [optionally a full batch].
    \item Define a loss $\mathcal{L}(\theta) = 0.5 [ Q_\theta(s'_t, a'_t) - (r'_t + \gamma \text{max}_a Q_{\theta'}(s'_{t+1}, a)) ]^2 $ [optionally averaged over the full batch]. Note the student network has parameters $\theta$ and the target network inside the max has parameters $\theta'$.
    \item Update the network parameters $\Delta \theta \propto - \frac{\partial \mathcal{L}(\theta)}{\partial \theta}$.
    \item Once we have repeated the above a sufficient number of times, set our target network to the student network, $\theta' \leftarrow \theta$.
\end{itemize}
%As indicated above, this procedure usually involves averaging over a full batch of experience when computing gradients.
%Experience can also be collected in batch, and usually `stale' experiences are periodically removed from $\mathcal{B}$.
This algorithm is effectively off-policy, since most experience in $\mathcal{B}$ was collected by a policy defined by an old set of parameters -- and the data in $\mathcal{B}$ can in fact be generated completely independently of the agent we are training.
Even though the DQN is more stable than naive deep Q-learning, an additional instability arises from the fact that $Q_{\text{max}}(s'_{t+1}) = \text{max}_a Q_{\theta'}(s'_{t+1}, a)$ uses the same Q values both to estimate which action is best and what the value of that action is, which leads to a positively biased estimate.
This can be mitigated by `double Q-learning' \citep{van2016deep}, where the student network selects the best action and the target network evaluates its value, $Q_{\text{max}}(s'_{t+1}) \leftarrow Q_{\theta'}(s'_{t+1}, \text{argmax}_a(Q_{\theta}(s'_{t+1}, a)))$.

While modern deep Q-learning has reached impressive performance across a range of machine learning settings \citep{mnih2013playing, lillicrap2015continuous, schaul2015prioritized, kalashnikov2018qt}, it is unclear whether the various modifications needed to stabilize the algorithm could be implemented in biological circuits.
This is perhaps the reason why neuroscience research using deep Q-learning has been relatively scarce, despite the prevalence of tabular Q-learning in the theoretical neuroscience literature.
An interesting exception is recent work by \citet{makino2023arithmetic}, which shows parallels between the values learned by a DQN and neural representations in mammalian cortex during a compositional behavioural task.
Additionally, the importance of experience replay in DQNs \citep{mnih2013playing, schaul2015prioritized} has close parallels to the proposal that hippocampal replay constitutes a form of experience replay \citep{mattar2018prioritized} -- although the latter was only analyzed in a tabular setting.

