\section{Problem setting}
\label{sec:problem_setting}

Here we provide a short introduction to the reinforcement learning problem in a discrete state and action space with a finite time horizon.
For a more general treatment, we refer to \citet{sutton2018reinforcement}.
In the discrete problem setting, the environment consists of states $s \in \mathcal{S}$, and the agent can take actions $a \in \mathcal{A}$.
The environment is characterized by transition and reward probabilities $p(s_{t+1}, r_t | s_t, a_t)$, where $r_t$ is the reward at time $t$.
We will generally assume that the reward is a function of the state and action, $r_t = r(s_t, a_t)$, where this denotes an expectation in the case of stochastic rewards and $r(s_t)$ denotes the expectation over actions.
We will further make the \emph{Markov assumption} that the next state only depends on the current state and action, $p(s_{t+1}, r_t | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = p(s_{t+1}, r_t | s_t, a_t)$.

We can now define a \emph{trajectory} $\tau = \{ s_t, a_t, r_t \}_{t = 0}^T$, where
\begin{equation}
    p(\tau) = p(s_0) \prod_{t = 0}^T p(s_{t+1}, r_t | s_t, a_t) p(a_t|s_t).
\end{equation}
$p(a_t|s_t)$ is the probability of taking action $a_t$ in state $s_t$, which is usually controlled by the agent and denoted a \emph{policy} $\pi(a_t|s_t)$.
The objective of the agent is to maximize the expected total discounted reward
\begin{align}
    \label{eq:RL_objective}
    J(\pi) &= \mathbb{E}_{\tau \sim p(\tau)} \left [ R_\tau \right ] = \mathbb{E}_{\tau \sim p(\tau)} \left [ \sum_{t=0}^T \gamma^t r_t | \tau \right ],
\end{align}
where $R_\tau := \sum_{t=0}^T \gamma^t r_t | \tau$ and we have written $J(\pi)$ since the policy uniquely specifies $J$ in a stationary environment.
In \Cref{eq:RL_objective}, $\gamma$ is a `discount factor', which stipulates that we should care more about immediate rewards than rewards far in the future.
We can provide three interpretations for this discount factor.
One is that people intrinsically care more about immediate reward than distant reward.
A second is that there is a finite probability $(1-\gamma)$ of the current `episode' or environment terminating or changing at each timestep, in which case we should weight putative future reward by the probability that we are still engaged in the task at that time.
The third view is that $\gamma$ simply provides a tool for reducing the variance of our learning methods, especially in temporally extended tasks.
This third view is most compatible with the fact that \emph{evaluation} of our RL agents at the end of training is generally done without any discounting.

Since $J(\pi$) depends on the policy of the agent, it is possible to search in the space of policies for one that maximizes $J$, which is the topic of reinforcement learning.
It is often assumed that the experience $\tau$ is generated by the agent acting according to its policy, and the resulting experience is then used to update the policy in a way that increases $J(\pi)$ (although note the existence of `off-policy' and `offline' RL; \citealp{levine2020offline}; \Cref{sec:off-policy}).



